{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import robotic as ry\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# some important constants\n",
    "i_just_want_to_see_the_BotOp_simulation = True\n",
    "\n",
    "want_to_see_uncropped_point_cloud = False\n",
    "want_to_see_cropped_point_cloud = not want_to_see_uncropped_point_cloud\n",
    "want_to_see_normals_of_objects = True\n",
    "want_to_see_separated_clusters = True\n",
    "want_to_see_best_grasp_for_each_object = True\n",
    "  \n",
    "  \n",
    "numOfObjects = 4\n",
    "pclColor = [0,0,255]\n",
    "botOpSpeed = 1\n",
    "voxelSize = 0.006\n",
    "degToleration = 6\n",
    "\n",
    "if i_just_want_to_see_the_BotOp_simulation:\n",
    "  want_to_see_uncropped_point_cloud, want_to_see_cropped_point_cloud, want_to_see_normals_of_objects, want_to_see_separated_clusters, want_to_see_best_grasp_for_each_object = False, False, False, False, False\n",
    "\n",
    "\n",
    "C = ry.Config()\n",
    "C.addFile('/home/vboxuser/Desktop/CS449/Migi/hw3/GFiles-HW3/environment.g')\n",
    "\n",
    "qHome = C.getJointState()\n",
    "C.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Additional object: a ball\n",
    "\n",
    "ballPos = C.getFrame(\"bin\").getPosition()\n",
    "ballPos[1] = ballPos[1] + 0.2\n",
    "ballPos[2] = ballPos[2] + 0.075\n",
    "\n",
    "ball = C.addFrame(\"ball\")\n",
    "ball.setShape(ry.ST.sphere, [.03])\n",
    "ball.setPosition(ballPos)\n",
    "ball.setMass(0.1)\n",
    "ball.setContact(1)\n",
    "\n",
    "C.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = ry.BotOp(C, useRealRobot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- bot.cpp:~BotOp:135(0) shutting down BotOp...\n",
      "-- simulation.cpp:~BotThreadedSim:57(0) shutting down SimThread\n",
      "-- simulation.cpp:~Simulation:148(0) shutting down Simulation\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  del bot\n",
    "  del komo\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Perception Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get point clouds for each camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 640, 3)\n",
      "(360, 640, 3)\n",
      "(360, 640, 3)\n",
      "(360, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "numOfCameras = 4\n",
    "cam = [None] * numOfCameras\n",
    "rgb = [None] * numOfCameras\n",
    "depth = [None] * numOfCameras\n",
    "pcl = [None] * numOfCameras\n",
    "\n",
    "\n",
    "for i in range(numOfCameras):\n",
    "  cam[i] = ry.CameraView(C)\n",
    "  cam[i].setCamera('camera'+str(i+1))\n",
    "  rgb[i], depth[i] = cam[i].computeImageAndDepth(C)\n",
    "  pcl[i] = ry.depthImage2PointCloud(depth[i], cam[i].getFxycxy())\n",
    "  print(pcl[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now normalise the points relative to world frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(numOfCameras):\n",
    "  curCam = C.getFrame(\"camera\"+str(i+1))  \n",
    "  curCamRotMat = curCam.getRotationMatrix()\n",
    "  curCamPos = curCam.getPosition()\n",
    "    \n",
    "  for h in range(pcl[i].shape[0]):    # Iterate over height\n",
    "    for w in range(pcl[i].shape[1]):  # Iterate over width\n",
    "      point = pcl[i][h, w]            # Get the 3D point [x, y, z]\n",
    "     \n",
    "      # Apply rotation\n",
    "      rotated_point = np.dot(curCamRotMat, point)  # Rotate the point\n",
    "      \n",
    "      # Apply translation\n",
    "      transformed_point = rotated_point + curCamPos  # Add the camera position\n",
    "      \n",
    "      pcl[i][h, w] = transformed_point      \n",
    "      \n",
    "\n",
    "# Visualize the point cloud\n",
    "if want_to_see_uncropped_point_cloud:\n",
    "  pclFrames = [None] * numOfCameras\n",
    "\n",
    "  for i in range(numOfCameras):\n",
    "    pclFrames[i] = C.addFrame('pcl'+str(i+1), 'world')\n",
    "    pclFrames[i].setPointCloud(pcl[i], pclColor)\n",
    "\n",
    "  C.view()\n",
    "  pclFrames = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we extract the ones inside the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "centre = C.getFrame(\"bin\").getPosition()\n",
    "centre[2] = centre[2] + 0.087  # Raise slightly to remove bin surface\n",
    "\n",
    "off = .1\n",
    "offsets = [0.3, 0.25, 0.05]\n",
    "\n",
    "bounding_box_min_x = centre[0] - offsets[0]\n",
    "bounding_box_max_x = centre[0] + offsets[0]\n",
    "\n",
    "bounding_box_min_y = centre[1] - offsets[1]\n",
    "bounding_box_max_y = centre[1] + offsets[1]\n",
    "\n",
    "bounding_box_min_z = centre[2] - offsets[2]\n",
    "bounding_box_max_z = centre[2] + offsets[2]\n",
    "\n",
    "\n",
    "for i in range(numOfCameras):\n",
    "  for h in range(pcl[i].shape[0]):  \n",
    "    for w in range(pcl[i].shape[1]):  \n",
    "      point = pcl[i][h, w] \n",
    "      eachX, eachY, eachZ = point  \n",
    "      \n",
    "      # Check if the point is within the bounding box\n",
    "      if not (bounding_box_min_x < eachX < bounding_box_max_x and\n",
    "              bounding_box_min_y < eachY < bounding_box_max_y and\n",
    "              bounding_box_min_z < eachZ < bounding_box_max_z):\n",
    "          pcl[i][h, w] = [0, 0, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now combine pcl from the 4 cameras and view the final bounded point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if want_to_see_cropped_point_cloud:\n",
    "  pclCombined = np.concatenate(pcl, axis=0)\n",
    "  pclFrame = C.addFrame('pcl', 'world')\n",
    "  pclFrame.setPointCloud(pclCombined, pclColor)\n",
    "\n",
    "  C.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.1 Antipodal Grasp Detection Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we do the normals usoing open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(921600, 3)\n",
      "Downsample the point cloud with a voxel of 0.05\n"
     ]
    }
   ],
   "source": [
    "# Reshape to a 2D array\n",
    "pclCombined_reshaped = pclCombined.reshape(-1, 3)  # Collapse the first two dimensions into one\n",
    "\n",
    "print(pclCombined_reshaped.shape)  # Should now be (1440 * 640, 3)\n",
    "\n",
    "# Convert to Open3D point cloud\n",
    "pcd = o3d.geometry.PointCloud()\n",
    "pcd.points = o3d.utility.Vector3dVector(pclCombined_reshaped)\n",
    "\n",
    "# #  Visualize the point cloud\n",
    "# o3d.visualization.draw_geometries([pcd])\n",
    "print(\"Downsample the point cloud with a voxel of 0.05\")\n",
    "downpcd = pcd.voxel_down_sample(voxel_size=voxelSize)\n",
    "downpcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "\n",
    "# # print list of normals\n",
    "# normals = np.asarray(downpcd.normals)\n",
    "# print(normals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we do the clustering using kmeans from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 2 2 ... 1 3 3]\n",
      "Cluster 0 has 1517 points\n",
      "Cluster 1 has 368 points\n",
      "Cluster 2 has 360 points\n",
      "Cluster 3 has 765 points\n",
      "Normals are already estimated.\n",
      "Normal magnitudes (first 10): [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "Are normals normalized?  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vboxuser/Desktop/CS449/rai_venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "# Cluster using kmeans from sklearn, there are 3 objects in the point cloud so we will use 3 clusters\n",
    "kmeans = KMeans(n_clusters=numOfObjects, random_state=0).fit(np.asarray(downpcd.points))\n",
    "print(kmeans.labels_)\n",
    "\n",
    "# Visualize the clusters\n",
    "clusters = []\n",
    "for i in range(numOfObjects):\n",
    "  cluster = downpcd.select_by_index(np.where(kmeans.labels_ == i)[0])\n",
    "  cluster.paint_uniform_color([(i == 0), (i == 1), (i == 2)])\n",
    "  \n",
    "  points = np.asarray(cluster.points)\n",
    "  print(\"Cluster\", i, \"has\", points.shape[0], \"points\")\n",
    "  \n",
    "  # Use Local Outlier Factor to detect outliers\n",
    "  lof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "  is_inlier = lof.fit_predict(points) > 0  # Inliers are labeled as 1, outliers as -1\n",
    "  \n",
    "  # Filter out outliers\n",
    "  filtered_points = points[is_inlier]\n",
    "  cluster.points = o3d.utility.Vector3dVector(filtered_points)\n",
    "  \n",
    "  # generate normals for the filtered points\n",
    "  cluster.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "  clusters.append(cluster)\n",
    "  \n",
    "\n",
    "  \n",
    "# Check if normals are estimated\n",
    "if downpcd.has_normals():\n",
    "    print(\"Normals are already estimated.\")\n",
    "else:\n",
    "    print(\"Normals are not estimated yet. Estimating now...\")\n",
    "    downpcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30))\n",
    "    print(\"Normals estimated.\")\n",
    "\n",
    "# Verify magnitudes of normals\n",
    "normals = np.asarray(downpcd.normals)\n",
    "magnitudes = np.linalg.norm(normals, axis=1)\n",
    "print(\"Normal magnitudes (first 10):\", magnitudes[:10])\n",
    "print(\"Are normals normalized? \", np.allclose(magnitudes, 1.0, atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixing normals for Cluster 1...\n",
      "Reorienting normals...\n",
      "Fixing normals for Cluster 2...\n",
      "Reorienting normals...\n",
      "Fixing normals for Cluster 3...\n",
      "Reorienting normals...\n",
      "Fixing normals for Cluster 4...\n",
      "Reorienting normals...\n"
     ]
    }
   ],
   "source": [
    "# Fix normals orientation by aligning them towards a camera position\n",
    "def reorient_normals(cluster):\n",
    "    print(\"Reorienting normals...\")\n",
    "    viewpoint = np.array([0, 0, 10])  # Replace with the actual camera or world origin\n",
    "    cluster.orient_normals_towards_camera_location(camera_location=viewpoint)\n",
    "\n",
    "# Reorient normals for each cluster\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(f\"Fixing normals for Cluster {i+1}...\")\n",
    "    reorient_normals(cluster)\n",
    "\n",
    "if want_to_see_normals_of_objects:\n",
    "    # Visualize the corrected normals\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        print(f\"Visualizing corrected normals for Cluster {i+1}...\")\n",
    "        o3d.visualization.draw_geometries([cluster], point_show_normal=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the centre of point clouds of each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "centre = [None] * numOfObjects\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "  bounding_box = cluster.get_axis_aligned_bounding_box()\n",
    "  centre[i] = bounding_box.get_center()\n",
    "  # print(centre[i])\n",
    "  # print(f\"Visualizing Cluster {i+1} with bounding box...\")\n",
    "  # o3d.visualization.draw_geometries([cluster, bounding_box])\n",
    "  \n",
    "  for point in clusters[i].points:\n",
    "    point[0] = point[0] - centre[i][0]\n",
    "    point[1] = point[1] - centre[i][1]\n",
    "    point[2] = point[2] - centre[i][2]\n",
    "\n",
    "\n",
    "# # First we will create frames for the objects\n",
    "# sprayBottle = C.addFrame(\"pcl_sprayBottle\")\n",
    "# sprayBottle.setPointCloud(clusters[0].points, [255, 0, 0])\n",
    "# sprayBottle.setPosition(centre[0])\n",
    "\n",
    "# rollingPin = C.addFrame(\"pcl_rollingPin\")\n",
    "# rollingPin.setPointCloud(clusters[1].points, [0, 255, 0])\n",
    "# rollingPin.setPosition(centre[1])\n",
    "\n",
    "# soapBox = C.addFrame(\"pcl_soapBox\")\n",
    "# soapBox.setPointCloud(clusters[2].points, [255, 255, 0])\n",
    "# soapBox.setPosition(centre[2])\n",
    "\n",
    "# ballAdditional = C.addFrame(\"pcl_ballAdditional\")\n",
    "# ballAdditional.setPointCloud(clusters[3].points, [255, 255, 0])\n",
    "# ballAdditional.setPosition(centre[3])\n",
    "\n",
    "# C.addFrame(\"vis_soap\", \"pcl_soapBox\").setShape(ry.ST.marker, [1]).setPosition(soapBox.getPosition())\n",
    "C.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we find antipodal grasp points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_antipodal_grasp_pairs(cluster):\n",
    "    points = np.asarray(cluster.points)\n",
    "    normals = np.asarray(cluster.normals)\n",
    "    \n",
    "    grasp_pairs = []\n",
    "    \n",
    "    # Iterate through all point pairs\n",
    "    for i in range(len(points)):\n",
    "        for j in range(i + 1, len(points)):\n",
    "            # Compute the angle between normals\n",
    "            normal_i = normals[i]\n",
    "            normal_j = normals[j]\n",
    "            dot_product = np.dot(normal_i, normal_j)\n",
    "            angle = np.arccos(dot_product)  # Angle in radians\n",
    "            \n",
    "            # Check if angle is close to 180 degrees (antipodal condition)\n",
    "            if np.abs(np.degrees(angle) - 180) < degToleration:  # 10-degree tolerance\n",
    "                grasp_pairs.append((i, j))\n",
    "    \n",
    "    return grasp_pairs\n",
    "\n",
    "def visualize_grasp_pairs(cluster, grasp_pairs):\n",
    "    points = np.asarray(cluster.points)\n",
    "    lines = []\n",
    "    line_colors = []\n",
    "\n",
    "    # Create lines connecting grasp pairs\n",
    "    for pair in grasp_pairs:\n",
    "        idx1, idx2 = pair\n",
    "        lines.append([idx1, idx2])\n",
    "        line_colors.append([1, 0, 0])  # Red lines for grasp pairs\n",
    "\n",
    "    # Create Open3D line set for visualization\n",
    "    line_set = o3d.geometry.LineSet()\n",
    "    line_set.points = o3d.utility.Vector3dVector(points)\n",
    "    line_set.lines = o3d.utility.Vector2iVector(lines)\n",
    "    line_set.colors = o3d.utility.Vector3dVector(line_colors)\n",
    "\n",
    "    # Visualize\n",
    "    print(\"Visualizing grasp pairs...\")\n",
    "    o3d.visualization.draw_geometries([cluster, line_set])\n",
    "\n",
    "def filter_opposite_side_grasps(cluster, grasp_pairs):\n",
    "    points = np.asarray(cluster.points)\n",
    "    normals = np.asarray(cluster.normals)\n",
    "\n",
    "    # Compute the bounding box of the object\n",
    "    min_bound = np.min(points, axis=0)\n",
    "    max_bound = np.max(points, axis=0)\n",
    "    object_extent = max_bound - min_bound\n",
    "\n",
    "    # Find the principal axis (longest dimension) of the object\n",
    "    # principal_axis_index = np.argmax(object_extent)  # 0 for x, 1 for y, 2 for z\n",
    "    principal_axis_index = 0  # 0 for x, 1 for y, 2 for z\n",
    "\n",
    "    valid_grasps = []\n",
    "\n",
    "    for pair in grasp_pairs:\n",
    "        idx1, idx2 = pair\n",
    "        point1, point2 = points[idx1], points[idx2]\n",
    "        normal1, normal2 = normals[idx1], normals[idx2]\n",
    "\n",
    "        # Check if the vector between the grasp points aligns with the principal axis\n",
    "        grasp_vector = point2 - point1\n",
    "        grasp_vector_unit = grasp_vector / np.linalg.norm(grasp_vector)\n",
    "        axis_vector = np.zeros(3)\n",
    "        axis_vector[principal_axis_index] = 1\n",
    "\n",
    "        alignment = np.abs(np.dot(grasp_vector_unit, axis_vector))  # Should be close to 1\n",
    "        if alignment < 0.9:  # Adjust threshold if necessary\n",
    "            continue\n",
    "\n",
    "        # Check if the normals face opposite directions\n",
    "        dot_product = np.dot(normal1, normal2)\n",
    "        if dot_product > -0.9:  # Ensure normals are nearly opposite\n",
    "            continue\n",
    "\n",
    "        # Add this pair as valid if both conditions are satisfied\n",
    "        valid_grasps.append(pair)\n",
    "\n",
    "    return valid_grasps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Cluster 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_55312/2628657640.py:14: RuntimeWarning: invalid value encountered in arccos\n",
      "  angle = np.arccos(dot_product)  # Angle in radians\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of potential grasp pairs in Cluster 1: 4\n",
      "Filtered grasp pairs for Cluster 1: 189\n",
      "Processing Cluster 2...\n",
      "Number of potential grasp pairs in Cluster 2: 4\n",
      "Filtered grasp pairs for Cluster 2: 53\n",
      "Processing Cluster 3...\n",
      "Number of potential grasp pairs in Cluster 3: 4\n",
      "Filtered grasp pairs for Cluster 3: 5\n",
      "Processing Cluster 4...\n",
      "Number of potential grasp pairs in Cluster 4: 4\n",
      "Filtered grasp pairs for Cluster 4: 90\n",
      "Best grasp pair for Cluster 1: (497, 1242) with distance closest to average 0.07315048466477161\n",
      "Best grasp pair for Cluster 2: (125, 320) with distance closest to average 0.04188608402982608\n",
      "Best grasp pair for Cluster 3: (60, 199) with distance closest to average 0.05902500893780619\n",
      "Best grasp pair for Cluster 4: (436, 619) with distance closest to average 0.0480100365511025\n"
     ]
    }
   ],
   "source": [
    "# Process each cluster\n",
    "grasp_pairs = [None] * len(clusters)\n",
    "common_distance = [None] * len(clusters)\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(f\"Processing Cluster {i+1}...\")\n",
    "    \n",
    "    grasp_pairs[i] = find_antipodal_grasp_pairs(cluster)\n",
    "    print(f\"Number of potential grasp pairs in Cluster {i+1}: {len(grasp_pairs)}\")\n",
    "    \n",
    "    grasp_pairs[i] = filter_opposite_side_grasps(cluster, grasp_pairs[i])\n",
    "    print(f\"Filtered grasp pairs for Cluster {i+1}: {len(grasp_pairs[i])}\")\n",
    "\n",
    "    \n",
    "    # print(f\"Visualizing grasp pairs for Cluster {i+1}...\")\n",
    "    # visualize_grasp_pairs(cluster, grasp_pairs[i])\n",
    "    \n",
    "# Finally, we need to select the best grasp pair for each object\n",
    "# We will select the pair with the avg distance between the two points\n",
    "# This is a simple heuristic, and there are more sophisticated methods available\n",
    "\n",
    "best_pairs = [None] * len(clusters)\n",
    "for i, cluster in enumerate(clusters):\n",
    "    points = np.asarray(cluster.points)\n",
    "    grasp_pair = grasp_pairs[i]\n",
    "    distances = []\n",
    "    \n",
    "    # Calculate distances for all grasp pairs\n",
    "    for pair in grasp_pair:\n",
    "        idx1, idx2 = pair\n",
    "        point1, point2 = points[idx1], points[idx2]\n",
    "        distance = np.linalg.norm(point2 - point1)\n",
    "        distances.append(distance)\n",
    "    \n",
    "    # Calculate the average distance\n",
    "    avg_distance = np.mean(distances)\n",
    "    best_pair = None\n",
    "    min_diff = float('inf')\n",
    "    \n",
    "    # Find the pair with the distance closest to the average distance\n",
    "    for pair, distance in zip(grasp_pair, distances):\n",
    "        diff = abs(distance - avg_distance)\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            best_pair = pair\n",
    "    \n",
    "    common_distance[i] = avg_distance\n",
    "    best_pairs[i] = best_pair\n",
    "\n",
    "    print(f\"Best grasp pair for Cluster {i+1}: {best_pair} with distance closest to average {avg_distance}\")\n",
    "    \n",
    "if want_to_see_best_grasp_for_each_object:\n",
    "    # Visualize the best grasp pair\n",
    "    best_pair = np.asarray(best_pair)\n",
    "    best_pair = best_pair.reshape(-1, 2)\n",
    "    visualize_grasp_pairs(cluster, best_pair)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.2 Now we do the KOMO problem, ig we run all the points and try to get the feasible ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a frame for each best pair\n",
    "frames = [None] * len(clusters)\n",
    "for i, cluster in enumerate(clusters):\n",
    "  midpoint = (clusters[i].points[best_pairs[i][0]] + clusters[i].points[best_pairs[i][1]]) / 2\n",
    "  \n",
    "  # Re adjust the midpoint\n",
    "  midpoint = midpoint + centre[i]\n",
    "  # print(\"Grasp pair for cluster\", i, \":\", clusters[i].points[best_pairs[i][0]], \"and\", clusters[i].points[best_pairs[i][1]])\n",
    "  # print(\"Midpoint:\", midpoint)\n",
    "  \n",
    "  \n",
    "  frame = C.addFrame(\"grasp_marker\" + str(i))\n",
    "  frame.setShape(ry.ST.marker, [.1])\n",
    "  frame.setPosition(midpoint)\n",
    "  # frame.setColor([1, 0, 0])\n",
    "  \n",
    "# We will also need to move the object to some point above the bin to avoid hitting the bin walls\n",
    "bufferPointPos = C.getFrame(\"bin\").getPosition()\n",
    "# bufferPointPos[0] = bufferPointPos[0] - 0.5\n",
    "bufferPointPos[1] = bufferPointPos[1] - 0.35\n",
    "bufferPointPos[2] = bufferPointPos[2] + 0.6\n",
    "\n",
    "buffer_point_frame = C.addFrame(\"buffer_point\")\n",
    "buffer_point_frame.setShape(ry.ST.marker, [.1])\n",
    "buffer_point_frame.setPosition(bufferPointPos)\n",
    "\n",
    "\n",
    "# also calc width of each object\n",
    "width = [None] * len(clusters)\n",
    "for i, cluster in enumerate(clusters):\n",
    "    points = np.asarray(cluster.points)\n",
    "    width[i] = np.linalg.norm(points[best_pairs[i][0]] - points[best_pairs[i][1]])\n",
    "\n",
    "  \n",
    "C.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  del bot\n",
    "  del komo\n",
    "except:\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ time: 0.000902, evals: 18, done: 1, feasible: 1, sos: 0.0405865, f: 0, ineq: 0, eq: 0.00149871 }\n"
     ]
    }
   ],
   "source": [
    "# Initialise BotOp and KOMO \n",
    "try:\n",
    "  del bot\n",
    "  del komo\n",
    "except:\n",
    "  pass\n",
    "C.setJointState(qHome)\n",
    "\n",
    "# Initialize BotOp for robot operation and Komo\n",
    "bot = ry.BotOp(C, useRealRobot=False)\n",
    "qHomeBot = bot.get_qHome()\n",
    "\n",
    "# komo = ry.KOMO(\n",
    "#   config=C, \n",
    "#   phases=1, \n",
    "#   slicesPerPhase=20, \n",
    "#   kOrder=0, \n",
    "#   enableCollisions=True\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(numOfObjects):\n",
    "  komo = ry.KOMO(C, 1, 1, 0, False)\n",
    "  \n",
    "  # Define KOMO for picking up object\n",
    "  komo.addObjective([], ry.FS.jointState, [], ry.OT.sos, [1e-1], qHome) #cost: close to qHome\n",
    "  komo.addObjective([], ry.FS.positionRel, ['l_gripper', 'grasp_marker' + str(i)], ry.OT.eq, [1e1], [0,0, -0.02]) #constraint: gripper position\n",
    "\n",
    "  # align gripper to grasp points\n",
    "  komo.addObjective(\n",
    "      times=[],\n",
    "      feature=ry.FS.scalarProductXX,\n",
    "      frames=['l_gripper', 'grasp_marker' + str(i)],\n",
    "      type=ry.OT.eq,\n",
    "      scale=[1e1],\n",
    "      target=[1]\n",
    "  )\n",
    "\n",
    "  komo.addObjective(\n",
    "      times=[],\n",
    "      feature=ry.FS.scalarProductYY,\n",
    "      frames=['l_gripper', 'grasp_marker' + str(i)],\n",
    "      type=ry.OT.eq,\n",
    "      scale=[1e1],\n",
    "      target=[1]\n",
    "  )\n",
    "\n",
    "  komo.addObjective(\n",
    "      times=[],\n",
    "      feature=ry.FS.scalarProductZZ,\n",
    "      frames=['l_gripper', 'grasp_marker' + str(i)],\n",
    "      type=ry.OT.eq,\n",
    "      scale=[1e1],\n",
    "      target=[1]\n",
    "  )\n",
    "\n",
    "  ret = ry.NLP_Solver(komo.nlp(), verbose=0) .solve()\n",
    "  print(ret)\n",
    "  if not ret.feasible:\n",
    "      raise Exception(\"No feasible path found for object \" + str(i))\n",
    "\n",
    "  # Retrieve the optimized path\n",
    "  path = komo.getPath()[0]\n",
    "\n",
    "  bot.moveTo(path, timeCost=1)\n",
    "\n",
    "  while bot.getTimeToEnd()>0:\n",
    "      bot.sync(C, .1)\n",
    "\n",
    "\n",
    "  \n",
    "  # grab the object using gripper\n",
    "  bot.gripperMove(ry._left, width=width[i]-0.07, speed=1)\n",
    "\n",
    "\n",
    "  # Intermediary step: Move the object to the buffer point\n",
    "\n",
    "  try:\n",
    "    del komo\n",
    "  except:\n",
    "    pass\n",
    "  komo = ry.KOMO(C, 1, 1, 0, False)\n",
    "  komo.addObjective([], ry.FS.jointState, [], ry.OT.sos, [1e-1], qHome) #cost: close to qHome\n",
    "  komo.addObjective(\n",
    "    times=[], \n",
    "    feature=ry.FS.positionDiff, \n",
    "    frames=['l_gripper', 'buffer_point'], \n",
    "    type=ry.OT.eq, scale=[1e1]\n",
    "  )\n",
    "\n",
    "  ret = ry.NLP_Solver(komo.nlp(), verbose=0) .solve()\n",
    "  intermediary = komo.getPath()[0]\n",
    "\n",
    "  bot.moveTo(intermediary, timeCost=botOpSpeed)\n",
    "  while bot.getTimeToEnd()>0:\n",
    "      bot.sync(C, .1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  # Now, we will move the object to the bin\n",
    "  try:\n",
    "    del komo\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "  komo = ry.KOMO(C, 1, 1, 0, False)\n",
    "  komo.addObjective([], ry.FS.jointState, [], ry.OT.sos, [1e-1], qHome) #cost: close to qHome\n",
    "\n",
    "  komo.addObjective(\n",
    "    times=[],\n",
    "    feature=ry.FS.positionRel,\n",
    "    frames=['l_gripper', 'bin-2'],\n",
    "    type=ry.OT.eq,\n",
    "    scale=[1e1],\n",
    "    target=[0, 0, 0.05]\n",
    "  )\n",
    "\n",
    "  ret = ry.NLP_Solver(komo.nlp(), verbose=0) .solve()\n",
    "  path_to_bin2 = komo.getPath()[0]\n",
    "\n",
    "  bot.moveTo(path_to_bin2, timeCost=botOpSpeed)\n",
    "\n",
    "  while bot.getTimeToEnd()>0:\n",
    "      bot.sync(C, .1)\n",
    "    \n",
    "  bot.gripperMove(ry._left, width=0.075, speed=1)\n",
    "  while not bot.gripperDone(ry._left):\n",
    "      bot.sync(C, .1)\n",
    "      \n",
    "\n",
    "  bot.moveTo(qHome, timeCost=5)\n",
    "  while bot.getTimeToEnd()>0:\n",
    "      bot.sync(C, .1)\n",
    "\n",
    "  # # now we move back to buffer point\n",
    "  # try:\n",
    "  #   del komo\n",
    "  # except:\n",
    "  #   pass\n",
    "  # komo = ry.KOMO(C, 1, 1, 0, False)\n",
    "  # komo.addObjective([], ry.FS.jointState, [], ry.OT.sos, [1e-1], qHome) #cost: close to qHome\n",
    "  # komo.addObjective(\n",
    "  #   times=[], \n",
    "  #   feature=ry.FS.positionDiff, \n",
    "  #   frames=['l_gripper', 'buffer_point'], \n",
    "  #   type=ry.OT.eq, scale=[1e1]\n",
    "  # )\n",
    "\n",
    "  # ret = ry.NLP_Solver(komo.nlp(), verbose=0) .solve()\n",
    "  # intermediary = komo.getPath()[0]\n",
    "\n",
    "  # bot.moveTo(intermediary, timeCost=botOpSpeed)\n",
    "  # while bot.getTimeToEnd()>0:\n",
    "  #     bot.sync(C, .1)\n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "C.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now KOMO move to other bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bot.moveTo(qHome, timeCost=botOpSpeed)\n",
    "# while bot.getTimeToEnd()>0:\n",
    "#     bot.sync(C, .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speed = 0.1\n",
    "# C.view()\n",
    "# qHomeJS = C.getJointState()\n",
    "# gripper_orig_pos = C.getFrame(\"l_gripper\").getPosition()\n",
    "# gripper_orig_quat = C.getFrame(\"l_gripper\").getQuaternion()\n",
    "# print(C.getFrameNames())\n",
    "\n",
    "# # initiliase bot and save home position\n",
    "# bot = ry.BotOp(C, useRealRobot=False)\n",
    "# qHome = bot.get_qHome()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
